{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.datasets.sevir.sevir import SEVIRLightningDataModule\n",
    "import time\n",
    "from omegaconf import OmegaConf\n",
    "cfg = OmegaConf.load(\"/home/vatsal/NWM/weatherforecasting/pipeline/datasets/sevir/config.yaml\").Dataset\n",
    "dm = SEVIRLightningDataModule(cfg)\n",
    "dm.prepare_data()\n",
    "setup_s = time.time()\n",
    "dm.setup()\n",
    "setup_e = time.time()\n",
    "print(f\"took : {setup_e - setup_s}\")\n",
    "\n",
    "l_s = time.time()\n",
    "loader = dm.train_dataloader()\n",
    "idx = 0\n",
    "for sample in loader:\n",
    "    idx += 1\n",
    "    if idx >= 200:break\n",
    "l_e = time.time()\n",
    "print(\"loading\", l_e - l_s)\n",
    "l_train_full = len(dm.train_dataloader())\n",
    "l_val_full = len(dm.val_dataloader())\n",
    "l_test_full = len(dm.test_dataloader())\n",
    "print(l_train_full, l_val_full, l_test_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.datasets.sevir.sevir import SEVIRLightningDataModule\n",
    "import time\n",
    "from omegaconf import OmegaConf\n",
    "cfg = OmegaConf.load(\"/home/vatsal/NWM/weatherforecasting/pipeline/datasets/sevir/fast_config.yaml\").Dataset\n",
    "dm = SEVIRLightningDataModule(cfg)\n",
    "dm.prepare_data()\n",
    "setup_s = time.time()\n",
    "dm.setup()\n",
    "setup_e = time.time()\n",
    "print(f\"took : {setup_e - setup_s}\")\n",
    "\n",
    "l_s = time.time()\n",
    "loader = dm.train_dataloader()\n",
    "idx = 0\n",
    "for sample in loader:\n",
    "    idx += 1\n",
    "    if idx >= 200:break\n",
    "l_e = time.time()\n",
    "print(\"loading\", l_e - l_s)\n",
    "l_train_fast = len(dm.train_dataloader())\n",
    "l_val_fast = len(dm.val_dataloader())\n",
    "l_test_fast = len(dm.test_dataloader())\n",
    "print(l_train_fast, l_val_fast, l_test_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l_train_fast/l_train_full)\n",
    "print(l_val_fast/l_val_full)\n",
    "print(l_test_fast/l_test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummpy = torch.randn(8, 52, 2304)\n",
    "from omegaconf import OmegaConf\n",
    "cfg = OmegaConf.load(\"/home/vatsal/NWM/weatherforecasting/experiments/pretrained_ae_dlinear_indc_indp/config.yaml\").dlinear\n",
    "model = DLinear(cfg)\n",
    "model2 = DLinear2(cfg)\n",
    "model.train()\n",
    "model2.train()\n",
    "state_dict = model.state_dict()\n",
    "model2.load_state_dict(state_dict)\n",
    "out1 = model(dummpy)\n",
    "out2 = model2(dummpy)\n",
    "#check all parameters are same\n",
    "flag = True\n",
    "for p1, p2 in zip(model.parameters(), model2.parameters()):\n",
    "    if not torch.allclose(p1, p2, atol=1e-4):\n",
    "        flag = False\n",
    "        break\n",
    "print(\"Parameters match:\", flag)\n",
    "print(out1.shape, out2.shape)\n",
    "print(torch.allclose(out1, out2, atol=1e-12))  #\n",
    "target = torch.randn(8, 48, 2304) \n",
    "loss1 = F.mse_loss(out1, target)\n",
    "loss2 = F.mse_loss(out2, target)\n",
    "\n",
    "loss1.backward()\n",
    "loss2.backward()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "optim2 = torch.optim.SGD(model2.parameters(), lr=1e-3)\n",
    "optim.step()\n",
    "optim2.step()\n",
    "\n",
    "# Check if the gradients are the same\n",
    "flag = True\n",
    "for (name1, p1), (name2, p2) in zip(model.named_parameters(), model2.named_parameters()):\n",
    "    if p1.grad is None or p2.grad is None:\n",
    "        print(f\"Gradient is None for: {name1}, {name2}\")\n",
    "        assert p1.grad is None and p2.grad is None, \"both gradients should be None\"\n",
    "        print(\"skipping gradient check for None gradients\")\n",
    "        continue\n",
    "    if not torch.allclose(p1.grad, p2.grad, atol=1e-4):\n",
    "        flag = False\n",
    "        break\n",
    "print(\"Gradients match:\", flag)\n",
    "\n",
    "out1_again = model(dummpy)\n",
    "out2_again = model2(dummpy)\n",
    "print(torch.allclose(out1_again, out2_again, atol=1e-12))  # should be True after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.act1 = nn.GELU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "            \n",
    "        self.act2 = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x)\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.act1(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        out += shortcut\n",
    "        return self.act2(out)\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, scale_factor):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=scale_factor, mode='nearest')\n",
    "        self.resblock = ResidualBlock(in_ch, out_ch, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resblock(self.upsample(x))\n",
    "\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, in_ch=1, latent_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder: 128 → 64 → 32 → 16 → 4 → 1\n",
    "        self.enc1 = ResidualBlock(in_ch,   64, stride=2)  # 128 → 64\n",
    "        self.enc2 = ResidualBlock(64,     128, stride=2)  # 64 → 32\n",
    "        self.enc3 = ResidualBlock(128,    256, stride=2)  # 32 → 16\n",
    "        self.enc4 = ResidualBlock(256,    512, stride=4)  # 16 → 4\n",
    "        self.enc5 = ResidualBlock(512,   1024, stride=4)  # 4 → 1\n",
    "\n",
    "        self.flatten = nn.Flatten()                     # (B,1024,1,1) → (B,1024)\n",
    "        self.fc_enc = nn.Linear(1024, latent_dim)\n",
    "\n",
    "        self.fc_dec = nn.Linear(latent_dim, 1024)\n",
    "        self.unflatten = nn.Unflatten(1, (1024, 1, 1))   # (B,1024) → (B,1024,1,1)\n",
    "        self.dec_init_conv = ResidualBlock(1024, 1024, stride=1)\n",
    "\n",
    "        # Decoder: 1 → 4 → 16 → 32 → 64 → 128\n",
    "        self.dec1 = UpsampleBlock(1024, 512, scale_factor=4)  # 1 → 4\n",
    "        self.dec2 = UpsampleBlock(512,  256, scale_factor=4)  # 4 → 16\n",
    "        self.dec3 = UpsampleBlock(256,  128, scale_factor=2)  # 16 → 32\n",
    "        self.dec4 = UpsampleBlock(128,   64, scale_factor=2)  # 32 → 64\n",
    "        \n",
    "        self.final_upsample = nn.Upsample(scale_factor=2, mode='nearest') # 64 → 128\n",
    "        self.final_conv = nn.Conv2d(64, in_ch, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.enc1(x)\n",
    "        x = self.enc2(x)\n",
    "        x = self.enc3(x)\n",
    "        x = self.enc4(x)\n",
    "        x = self.enc5(x)\n",
    "        x = self.flatten(x)\n",
    "        z = self.fc_enc(x)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self.fc_dec(z)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.dec_init_conv(x)\n",
    "        x = self.dec1(x)\n",
    "        x = self.dec2(x)\n",
    "        x = self.dec3(x)\n",
    "        x = self.dec4(x)\n",
    "        x = self.final_upsample(x)\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        reconstruction = self.decode(z)\n",
    "        return reconstruction, z\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import torch\n",
    "    model = ConvAutoencoder()\n",
    "    x = torch.randn(8, 1, 128, 128)  # Batch of 8 images with 1 channel, 128x128 size\n",
    "    recon, z = model(x)\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Reconstruction shape:\", recon.shape)\n",
    "    print(\"Latent vector shape:\", z.shape)\n",
    "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    params_in_million = params / 1e6\n",
    "    print(f\"Total trainable parameters: {params_in_million:.2f} million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/vatsal/NWM/sevir_lr/data/vil/2017/SEVIR_VIL_RANDOMEVENTS_2017_0501_0831.h5\"\n",
    "import h5py\n",
    "data = h5py.File(path, 'r')\n",
    "vil = data['vil'][:]\n",
    "print(\"Shape of VIL data:\", vil.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "os.environ['WANDB_API_KEY'] = '041eda3850f131617ee1d1c9714e6230c6ac4772'\n",
    "datetime.datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "exp_name = \"test_run_xyz\"\n",
    "save_dir = os.path.join(\"experiments\", \"test\", exp_name + datetime.datetime.now().strftime(\"_%Y%m%d_%H%M%S\"))\n",
    "version = \"v1.0\"\n",
    "logger = WandbLogger(project=\"delete_it\", name=\"test_run\", save_dir=save_dir, version=version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.datasets.sevire.sevir import SEVIRLightningDataModule\n",
    "data = SEVIRLightningDataModule(dataset_name=\"sevir_lr\", num_workers=0, batch_size=1, seq_len=1, stride=1, layout='NTHW')\n",
    "data.setup()\n",
    "data.prepare_data()\n",
    "train_loader = data.train_dataloader()\n",
    "sample1 = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.datasets.sevire.sevir import SEVIRLightningDataModule\n",
    "data2 = SEVIRLightningDataModule(dataset_name=\"sevir_lr\", num_workers=8, batch_size=8, seq_len=12, stride=20, layout='NTHW')\n",
    "data2.setup()\n",
    "data2.prepare_data()\n",
    "train_loader2 = data2.train_dataloader()\n",
    "sample2 = next(iter(train_loader2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2_dict = {}\n",
    "print(len(train_loader2))\n",
    "from tqdm import tqdm\n",
    "for epoch in range(2):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    if epoch == 0:\n",
    "        for idx, sample in tqdm(enumerate(train_loader2)):\n",
    "            data = sample['vil']\n",
    "            data2_dict[data] = idx\n",
    "    else:\n",
    "        for sample in tqdm(train_loader2):\n",
    "            data = sample['vil']\n",
    "            idx = -1\n",
    "            for i, d in enumerate(data2_dict.keys()):\n",
    "                if torch.allclose(d, data, atol=1e-12):\n",
    "                    idx = i\n",
    "                    break\n",
    "            print(f\"Found data at index: {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sample in tqdm(enumerate(train_loader2)):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.datasets.sevire.sevir import SEVIRLightningDataModule\n",
    "data3 = SEVIRLightningDataModule(dataset_name=\"sevir_lr\", num_workers=0, batch_size=8, seq_len=1, stride=1, layout='NTHW')\n",
    "data3.setup()\n",
    "data3.prepare_data()\n",
    "train_loader3 = data2.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample2, sample3 in zip(train_loader2, train_loader3):\n",
    "    data2 = sample2['vil']\n",
    "    data3 = sample3['vil']\n",
    "    if not torch.allclose(data2, data3, atol=1e-12):\n",
    "        print(\"Data mismatch found!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample3 = next(iter(train_loader3))\n",
    "import torch\n",
    "torch.allclose(sample2['vil'], sample3['vil'], atol=1e-12) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = []\n",
    "import torch\n",
    "for sample in train_loader2:\n",
    "    data = sample['vil']\n",
    "    datas.append(data)\n",
    "print(len(datas))\n",
    "for sample in train_loader3:\n",
    "    data = sample['vil']\n",
    "    idx = -1\n",
    "    for i, d in enumerate(datas):\n",
    "        if torch.allclose(data, d, atol=1e-12):\n",
    "            idx = i\n",
    "            break\n",
    "    if idx != -1:\n",
    "        print(f\"Found matching data at index {idx}\")\n",
    "    else:\n",
    "        print(\"No matching data found _______________________________-\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = []\n",
    "import torch\n",
    "for sample in train_loader2:\n",
    "    data = sample['vil']\n",
    "    datas.append(data)\n",
    "print(len(datas))\n",
    "for sample in train_loader2:\n",
    "    data = sample['vil']\n",
    "    idx = -1\n",
    "    for i, d in enumerate(datas):\n",
    "        if torch.allclose(data, d, atol=1e-12):\n",
    "            idx = i\n",
    "            break\n",
    "    if idx != -1:\n",
    "        print(f\"Found matching data at index {idx}\")\n",
    "    else:\n",
    "        print(\"No matching data found _______________________________-\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = []\n",
    "import torch\n",
    "for sample in train_loader2:\n",
    "    data = sample['vil']\n",
    "    for bdata in data:\n",
    "        datas.append(bdata)\n",
    "print(len(datas))\n",
    "for sample in train_loader3:\n",
    "    data = sample['vil']\n",
    "    for bdata in data:\n",
    "        idx = -1\n",
    "        for i, d in enumerate(datas):\n",
    "            if torch.allclose(bdata, d, atol=1e-12):\n",
    "                idx = i\n",
    "                break\n",
    "        if idx != -1:\n",
    "            print(f\"Found matching data at index {idx}\")\n",
    "        else:\n",
    "            print(\"No matching data found _______________________________-\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "\n",
    "def cleanup_checkpoints_and_wandb(root_dir):\n",
    "    total_freed = 0\n",
    "    # Delete .ckpt files (keep most recent)\n",
    "    for dirpath, dirnames, _ in os.walk(root_dir):\n",
    "        if os.path.basename(dirpath) == \"checkpoints\":\n",
    "            ckpts = glob.glob(os.path.join(dirpath, \"*.ckpt\"))\n",
    "            if len(ckpts) <= 1:\n",
    "                continue\n",
    "            ckpts = [(f, os.path.getmtime(f)) for f in ckpts]\n",
    "            ckpts.sort(key=lambda x: x[1], reverse=True)\n",
    "            to_delete = ckpts[1:]  # keep most recent\n",
    "\n",
    "            for fpath, _ in to_delete:\n",
    "                try:\n",
    "                    size_mb = os.path.getsize(fpath) / (1024 * 1024)\n",
    "                    os.remove(fpath)\n",
    "                    total_freed += size_mb\n",
    "                    print(f\"Deleted .ckpt: {fpath} ({size_mb:.2f} MB)\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to delete {fpath}: {e}\")\n",
    "\n",
    "    # Delete all .wandb files\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for fname in filenames:\n",
    "            if fname.endswith(\".wandb\"):\n",
    "                fpath = os.path.join(dirpath, fname)\n",
    "                try:\n",
    "                    size_mb = os.path.getsize(fpath) / (1024 * 1024)\n",
    "                    os.remove(fpath)\n",
    "                    total_freed += size_mb\n",
    "                    print(f\"Deleted .wandb: {fpath} ({size_mb:.2f} MB)\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to delete {fpath}: {e}\")\n",
    "\n",
    "    print(f\"Total space freed: {total_freed:.2f} MB\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cleanup_checkpoints_and_wandb(\"/home/vatsal/NWM/weatherforecasting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_total_png_size(root_dir):\n",
    "    total_size = 0\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for fname in filenames:\n",
    "            if fname.endswith(\".png\"):\n",
    "                fpath = os.path.join(dirpath, fname)\n",
    "                try:\n",
    "                    total_size += os.path.getsize(fpath)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {fpath}: {e}\")\n",
    "    print(f\"Total .png size: {total_size / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    get_total_png_size(\"/home/vatsal/NWM/weatherforecasting/experiments/pretrained_ae_linear_sevir/outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total .ckpt size: 5.23 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_ckpt_total_size(root_dir):\n",
    "    total_size = 0\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for fname in filenames:\n",
    "            if fname.endswith('.ckpt'):\n",
    "                fpath = os.path.join(dirpath, fname)\n",
    "                total_size += os.path.getsize(fpath)\n",
    "    return total_size\n",
    "\n",
    "def format_size(bytes_size):\n",
    "    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if bytes_size < 1024:\n",
    "            return f\"{bytes_size:.2f} {unit}\"\n",
    "        bytes_size /= 1024\n",
    "    return f\"{bytes_size:.2f} PB\"\n",
    "\n",
    "root = \"/home/vatsal/NWM\"\n",
    "total = get_ckpt_total_size(root)\n",
    "print(\"Total .ckpt size:\", format_size(total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vatsal/NWM/weatherforecasting: 20.60 GB\n",
      "/home/vatsal/NWM/weather: 11.77 GB\n",
      "/home/vatsal/NWM/sevir_lr: 8.77 GB\n",
      "/home/vatsal/NWM/PreDiff: 0.36 GB\n",
      "/home/vatsal/NWM/DiffCast: 0.16 GB\n",
      "/home/vatsal/NWM/AlphaPre: 0.00 GB\n",
      "/home/vatsal/NWM/.vscode: 0.00 GB\n",
      "/home/vatsal/NWM/wandb: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    for dirpath, _, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fpath = os.path.join(dirpath, f)\n",
    "            try:\n",
    "                if not os.path.islink(fpath) and os.path.isfile(fpath):\n",
    "                    total += os.path.getsize(fpath)\n",
    "            except:\n",
    "                continue\n",
    "    return total\n",
    "\n",
    "def list_top_dirs(root_dir, top_n=20):\n",
    "    dir_sizes = []\n",
    "    for entry in os.scandir(root_dir):\n",
    "        if entry.is_dir(follow_symlinks=False):\n",
    "            size = get_dir_size(entry.path)\n",
    "            dir_sizes.append((entry.path, size))\n",
    "    \n",
    "    dir_sizes.sort(key=lambda x: x[1], reverse=True)\n",
    "    for path, size in dir_sizes[:top_n]:\n",
    "        print(f\"{path}: {size / (1024**3):.2f} GB\")\n",
    "\n",
    "# Example usage\n",
    "list_top_dirs(\"/home/vatsal/NWM\", top_n=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earthformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
