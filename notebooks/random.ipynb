{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.datasets.sevir.sevir import SEVIRLightningDataModule\n",
    "import time\n",
    "from omegaconf import OmegaConf\n",
    "cfg = OmegaConf.load(\"/home/vatsal/NWM/weatherforecasting/pipeline/datasets/sevir/config.yaml\").Dataset\n",
    "dm = SEVIRLightningDataModule(cfg)\n",
    "dm.prepare_data()\n",
    "setup_s = time.time()\n",
    "dm.setup()\n",
    "setup_e = time.time()\n",
    "print(f\"took : {setup_e - setup_s}\")\n",
    "\n",
    "l_s = time.time()\n",
    "loader = dm.train_dataloader()\n",
    "idx = 0\n",
    "for sample in loader:\n",
    "    idx += 1\n",
    "    if idx >= 200:break\n",
    "l_e = time.time()\n",
    "print(\"loading\", l_e - l_s)\n",
    "l_train_full = len(dm.train_dataloader())\n",
    "l_val_full = len(dm.val_dataloader())\n",
    "l_test_full = len(dm.test_dataloader())\n",
    "print(l_train_full, l_val_full, l_test_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.datasets.sevir.sevir import SEVIRLightningDataModule\n",
    "import time\n",
    "from omegaconf import OmegaConf\n",
    "cfg = OmegaConf.load(\"/home/vatsal/NWM/weatherforecasting/pipeline/datasets/sevir/fast_config.yaml\").Dataset\n",
    "dm = SEVIRLightningDataModule(cfg)\n",
    "dm.prepare_data()\n",
    "setup_s = time.time()\n",
    "dm.setup()\n",
    "setup_e = time.time()\n",
    "print(f\"took : {setup_e - setup_s}\")\n",
    "\n",
    "l_s = time.time()\n",
    "loader = dm.train_dataloader()\n",
    "idx = 0\n",
    "for sample in loader:\n",
    "    idx += 1\n",
    "    if idx >= 200:break\n",
    "l_e = time.time()\n",
    "print(\"loading\", l_e - l_s)\n",
    "l_train_fast = len(dm.train_dataloader())\n",
    "l_val_fast = len(dm.val_dataloader())\n",
    "l_test_fast = len(dm.test_dataloader())\n",
    "print(l_train_fast, l_val_fast, l_test_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l_train_fast/l_train_full)\n",
    "print(l_val_fast/l_val_full)\n",
    "print(l_test_fast/l_test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummpy = torch.randn(8, 52, 2304)\n",
    "from omegaconf import OmegaConf\n",
    "cfg = OmegaConf.load(\"/home/vatsal/NWM/weatherforecasting/experiments/pretrained_ae_dlinear_indc_indp/config.yaml\").dlinear\n",
    "model = DLinear(cfg)\n",
    "model2 = DLinear2(cfg)\n",
    "model.train()\n",
    "model2.train()\n",
    "state_dict = model.state_dict()\n",
    "model2.load_state_dict(state_dict)\n",
    "out1 = model(dummpy)\n",
    "out2 = model2(dummpy)\n",
    "#check all parameters are same\n",
    "flag = True\n",
    "for p1, p2 in zip(model.parameters(), model2.parameters()):\n",
    "    if not torch.allclose(p1, p2, atol=1e-4):\n",
    "        flag = False\n",
    "        break\n",
    "print(\"Parameters match:\", flag)\n",
    "print(out1.shape, out2.shape)\n",
    "print(torch.allclose(out1, out2, atol=1e-12))  #\n",
    "target = torch.randn(8, 48, 2304) \n",
    "loss1 = F.mse_loss(out1, target)\n",
    "loss2 = F.mse_loss(out2, target)\n",
    "\n",
    "loss1.backward()\n",
    "loss2.backward()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "optim2 = torch.optim.SGD(model2.parameters(), lr=1e-3)\n",
    "optim.step()\n",
    "optim2.step()\n",
    "\n",
    "# Check if the gradients are the same\n",
    "flag = True\n",
    "for (name1, p1), (name2, p2) in zip(model.named_parameters(), model2.named_parameters()):\n",
    "    if p1.grad is None or p2.grad is None:\n",
    "        print(f\"Gradient is None for: {name1}, {name2}\")\n",
    "        assert p1.grad is None and p2.grad is None, \"both gradients should be None\"\n",
    "        print(\"skipping gradient check for None gradients\")\n",
    "        continue\n",
    "    if not torch.allclose(p1.grad, p2.grad, atol=1e-4):\n",
    "        flag = False\n",
    "        break\n",
    "print(\"Gradients match:\", flag)\n",
    "\n",
    "out1_again = model(dummpy)\n",
    "out2_again = model2(dummpy)\n",
    "print(torch.allclose(out1_again, out2_again, atol=1e-12))  # should be True after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.act1 = nn.GELU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "            \n",
    "        self.act2 = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x)\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.act1(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        out += shortcut\n",
    "        return self.act2(out)\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, scale_factor):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=scale_factor, mode='nearest')\n",
    "        self.resblock = ResidualBlock(in_ch, out_ch, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resblock(self.upsample(x))\n",
    "\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, in_ch=1, latent_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder: 128 → 64 → 32 → 16 → 4 → 1\n",
    "        self.enc1 = ResidualBlock(in_ch,   64, stride=2)  # 128 → 64\n",
    "        self.enc2 = ResidualBlock(64,     128, stride=2)  # 64 → 32\n",
    "        self.enc3 = ResidualBlock(128,    256, stride=2)  # 32 → 16\n",
    "        self.enc4 = ResidualBlock(256,    512, stride=4)  # 16 → 4\n",
    "        self.enc5 = ResidualBlock(512,   1024, stride=4)  # 4 → 1\n",
    "\n",
    "        self.flatten = nn.Flatten()                     # (B,1024,1,1) → (B,1024)\n",
    "        self.fc_enc = nn.Linear(1024, latent_dim)\n",
    "\n",
    "        self.fc_dec = nn.Linear(latent_dim, 1024)\n",
    "        self.unflatten = nn.Unflatten(1, (1024, 1, 1))   # (B,1024) → (B,1024,1,1)\n",
    "        self.dec_init_conv = ResidualBlock(1024, 1024, stride=1)\n",
    "\n",
    "        # Decoder: 1 → 4 → 16 → 32 → 64 → 128\n",
    "        self.dec1 = UpsampleBlock(1024, 512, scale_factor=4)  # 1 → 4\n",
    "        self.dec2 = UpsampleBlock(512,  256, scale_factor=4)  # 4 → 16\n",
    "        self.dec3 = UpsampleBlock(256,  128, scale_factor=2)  # 16 → 32\n",
    "        self.dec4 = UpsampleBlock(128,   64, scale_factor=2)  # 32 → 64\n",
    "        \n",
    "        self.final_upsample = nn.Upsample(scale_factor=2, mode='nearest') # 64 → 128\n",
    "        self.final_conv = nn.Conv2d(64, in_ch, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.enc1(x)\n",
    "        x = self.enc2(x)\n",
    "        x = self.enc3(x)\n",
    "        x = self.enc4(x)\n",
    "        x = self.enc5(x)\n",
    "        x = self.flatten(x)\n",
    "        z = self.fc_enc(x)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self.fc_dec(z)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.dec_init_conv(x)\n",
    "        x = self.dec1(x)\n",
    "        x = self.dec2(x)\n",
    "        x = self.dec3(x)\n",
    "        x = self.dec4(x)\n",
    "        x = self.final_upsample(x)\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        reconstruction = self.decode(z)\n",
    "        return reconstruction, z\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import torch\n",
    "    model = ConvAutoencoder()\n",
    "    x = torch.randn(8, 1, 128, 128)  # Batch of 8 images with 1 channel, 128x128 size\n",
    "    recon, z = model(x)\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Reconstruction shape:\", recon.shape)\n",
    "    print(\"Latent vector shape:\", z.shape)\n",
    "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    params_in_million = params / 1e6\n",
    "    print(f\"Total trainable parameters: {params_in_million:.2f} million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/vatsal/NWM/sevir_lr/data/vil/2017/SEVIR_VIL_RANDOMEVENTS_2017_0501_0831.h5\"\n",
    "import h5py\n",
    "data = h5py.File(path, 'r')\n",
    "vil = data['vil'][:]\n",
    "print(\"Shape of VIL data:\", vil.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "os.environ['WANDB_API_KEY'] = '041eda3850f131617ee1d1c9714e6230c6ac4772'\n",
    "datetime.datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "exp_name = \"test_run_xyz\"\n",
    "save_dir = os.path.join(\"experiments\", \"test\", exp_name + datetime.datetime.now().strftime(\"_%Y%m%d_%H%M%S\"))\n",
    "version = \"v1.0\"\n",
    "logger = WandbLogger(project=\"delete_it\", name=\"test_run\", save_dir=save_dir, version=version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.datasets.sevire.sevir import SEVIRLightningDataModule\n",
    "data = SEVIRLightningDataModule(dataset_name=\"sevir_lr\", num_workers=0, batch_size=1, seq_len=1, stride=1, layout='NTHW')\n",
    "data.setup()\n",
    "data.prepare_data()\n",
    "train_loader = data.train_dataloader()\n",
    "sample1 = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.datasets.sevire.sevir import SEVIRLightningDataModule\n",
    "data2 = SEVIRLightningDataModule(dataset_name=\"sevir_lr\", num_workers=8, batch_size=8, seq_len=12, stride=20, layout='NTHW')\n",
    "data2.setup()\n",
    "data2.prepare_data()\n",
    "train_loader2 = data2.train_dataloader()\n",
    "sample2 = next(iter(train_loader2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2_dict = {}\n",
    "print(len(train_loader2))\n",
    "from tqdm import tqdm\n",
    "for epoch in range(2):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    if epoch == 0:\n",
    "        for idx, sample in tqdm(enumerate(train_loader2)):\n",
    "            data = sample['vil']\n",
    "            data2_dict[data] = idx\n",
    "    else:\n",
    "        for sample in tqdm(train_loader2):\n",
    "            data = sample['vil']\n",
    "            idx = -1\n",
    "            for i, d in enumerate(data2_dict.keys()):\n",
    "                if torch.allclose(d, data, atol=1e-12):\n",
    "                    idx = i\n",
    "                    break\n",
    "            print(f\"Found data at index: {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.datasets.sevire.sevir import SEVIRLightningDataModule\n",
    "data3 = SEVIRLightningDataModule(dataset_name=\"sevir_lr\", num_workers=0, batch_size=8, seq_len=1, stride=1, layout='NTHW')\n",
    "data3.setup()\n",
    "data3.prepare_data()\n",
    "train_loader3 = data2.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample2, sample3 in zip(train_loader2, train_loader3):\n",
    "    data2 = sample2['vil']\n",
    "    data3 = sample3['vil']\n",
    "    if not torch.allclose(data2, data3, atol=1e-12):\n",
    "        print(\"Data mismatch found!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample3 = next(iter(train_loader3))\n",
    "import torch\n",
    "torch.allclose(sample2['vil'], sample3['vil'], atol=1e-12) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = []\n",
    "import torch\n",
    "for sample in train_loader2:\n",
    "    data = sample['vil']\n",
    "    datas.append(data)\n",
    "print(len(datas))\n",
    "for sample in train_loader3:\n",
    "    data = sample['vil']\n",
    "    idx = -1\n",
    "    for i, d in enumerate(datas):\n",
    "        if torch.allclose(data, d, atol=1e-12):\n",
    "            idx = i\n",
    "            break\n",
    "    if idx != -1:\n",
    "        print(f\"Found matching data at index {idx}\")\n",
    "    else:\n",
    "        print(\"No matching data found _______________________________-\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = []\n",
    "import torch\n",
    "for sample in train_loader2:\n",
    "    data = sample['vil']\n",
    "    datas.append(data)\n",
    "print(len(datas))\n",
    "for sample in train_loader2:\n",
    "    data = sample['vil']\n",
    "    idx = -1\n",
    "    for i, d in enumerate(datas):\n",
    "        if torch.allclose(data, d, atol=1e-12):\n",
    "            idx = i\n",
    "            break\n",
    "    if idx != -1:\n",
    "        print(f\"Found matching data at index {idx}\")\n",
    "    else:\n",
    "        print(\"No matching data found _______________________________-\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = []\n",
    "import torch\n",
    "for sample in train_loader2:\n",
    "    data = sample['vil']\n",
    "    for bdata in data:\n",
    "        datas.append(bdata)\n",
    "print(len(datas))\n",
    "for sample in train_loader3:\n",
    "    data = sample['vil']\n",
    "    for bdata in data:\n",
    "        idx = -1\n",
    "        for i, d in enumerate(datas):\n",
    "            if torch.allclose(bdata, d, atol=1e-12):\n",
    "                idx = i\n",
    "                break\n",
    "        if idx != -1:\n",
    "            print(f\"Found matching data at index {idx}\")\n",
    "        else:\n",
    "            print(\"No matching data found _______________________________-\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free disk space: 0.00 GB\n",
      "Deleted .wandb: /home/vatsal/NWM/weatherforecasting/experiments/ae_v2_2/outputs/ae_v2_16x16x16_lin_disc/wandb/run-20250729_233021-lrp42ayk/run-lrp42ayk.wandb (1.27 MB)\n",
      "Deleted .wandb: /home/vatsal/NWM/weatherforecasting/experiments/ae_v2_2/outputs/ae_v2_16x16x16_lin_disc/wandb/run-20250730_050408-o9kuedtz/run-o9kuedtz.wandb (407.94 MB)\n",
      "Deleted .wandb: /home/vatsal/NWM/weatherforecasting/experiments/ae_v2_2/outputs/ae_v2_16x16x16_lin_disc/wandb/run-20250729_232856-eupzfe50/run-eupzfe50.wandb (0.84 MB)\n",
      "Deleted .wandb: /home/vatsal/NWM/weatherforecasting/experiments/ae_v2_2/outputs/ae_v2_16x16x16_lin_disc/wandb/run-20250729_233150-o9kuedtz/run-o9kuedtz.wandb (788.28 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted .wandb: /home/vatsal/NWM/weatherforecasting/experiments/ae_v2/outputs/ae_v2_64x8x8_lin_disc/wandb/run-20250729_232128-7qamm6uz/run-7qamm6uz.wandb (2178.28 MB)\n",
      "Deleted .wandb: /home/vatsal/NWM/weatherforecasting/experiments/ae_v2/outputs/ae_v2_64x8x8_lin/wandb/run-20250729_231544-hun23jye/run-hun23jye.wandb (0.10 MB)\n",
      "Deleted .wandb: /home/vatsal/NWM/weatherforecasting/experiments/ae_v2/outputs/ae_v2_64x8x8_lin/wandb/run-20250729_231642-hun23jye/run-hun23jye.wandb (0.10 MB)\n",
      "Total space freed: 3376.81 MB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDisk space is sufficient, no cleanup needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "def get_free_disk_space_gb(path):\n",
    "    \"\"\"Returns free disk space in GB for the given path.\"\"\"\n",
    "    stat = shutil.disk_usage(path)\n",
    "    return stat.free / (1024 ** 3)\n",
    "\n",
    "def cleanup_checkpoints_and_wandb(root_dir):\n",
    "    total_freed = 0\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        # Delete .wandb files\n",
    "        for fname in filenames:\n",
    "            if fname.endswith(\".wandb\"):\n",
    "                fpath = os.path.join(dirpath, fname)\n",
    "                try:\n",
    "                    size_mb = os.path.getsize(fpath) / (1024 * 1024)\n",
    "                    os.remove(fpath)\n",
    "                    total_freed += size_mb\n",
    "                    print(f\"Deleted .wandb: {fpath} ({size_mb:.2f} MB)\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to delete {fpath}: {e}\")\n",
    "        # Delete .ckpt files if more than 2 in a directory\n",
    "        ckpt_files = [f for f in filenames if f.endswith(\".ckpt\")]\n",
    "        if len(ckpt_files) > 2:\n",
    "            # Sort by modification time, keep the 2 newest\n",
    "            ckpt_paths = [os.path.join(dirpath, f) for f in ckpt_files]\n",
    "            ckpt_paths.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "            for fpath in ckpt_paths[2:]:\n",
    "                try:\n",
    "                    size_mb = os.path.getsize(fpath) / (1024 * 1024)\n",
    "                    os.remove(fpath)\n",
    "                    total_freed += size_mb\n",
    "                    print(f\"Deleted .ckpt: {fpath} ({size_mb:.2f} MB)\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to delete {fpath}: {e}\")\n",
    "    print(f\"Total space freed: {total_freed:.2f} MB\")\n",
    "\n",
    "root = \"/home/vatsal/NWM/weatherforecasting\"\n",
    "while True:\n",
    "    free_gb = get_free_disk_space_gb(root)\n",
    "    print(f\"Free disk space: {free_gb:.2f} GB\")\n",
    "    if free_gb < 3:\n",
    "        cleanup_checkpoints_and_wandb(root)\n",
    "    else:\n",
    "        print(\"Disk space is sufficient, no cleanup needed.\")\n",
    "    time.sleep(200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([8, 1, 128, 128])\n",
      "Latent vector shape: torch.Size([8, 128, 4, 4])\n",
      "Reconstruction shape: torch.Size([8, 1, 128, 128])\n",
      "Total trainable parameters: 5.85 million\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=3, s=1, p=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, k, s, p, bias=False)\n",
    "        self.norm1 = nn.GroupNorm(min(32, out_ch), out_ch)\n",
    "        self.act1 = nn.GELU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, k, 1, p, bias=False)\n",
    "        self.norm2 = nn.GroupNorm(min(32, out_ch), out_ch)\n",
    "        self.act2 = nn.GELU()\n",
    "        self.shortcut = (\n",
    "            nn.Conv2d(in_ch, out_ch, 1, s, bias=False)\n",
    "            if in_ch != out_ch or s != 1\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.shortcut(x)\n",
    "        out = self.act1(self.norm1(self.conv1(x)))\n",
    "        out = self.norm2(self.conv2(out))\n",
    "        return self.act2(out + res)\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, channels, num_heads=8):\n",
    "        super().__init__()\n",
    "        assert channels % num_heads == 0\n",
    "        self.norm = nn.GroupNorm(min(32, channels), channels)\n",
    "        self.q = nn.Conv2d(channels, channels, 1)\n",
    "        self.k = nn.Conv2d(channels, channels, 1)\n",
    "        self.v = nn.Conv2d(channels, channels, 1)\n",
    "        self.proj = nn.Conv2d(channels, channels, 1)\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = channels // num_heads\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        h = self.norm(x)\n",
    "        q = self.q(h).reshape(B, self.num_heads, self.head_dim, H*W).permute(0,1,3,2)\n",
    "        k = self.k(h).reshape(B, self.num_heads, self.head_dim, H*W).permute(0,1,2,3)\n",
    "        v = self.v(h).reshape(B, self.num_heads, self.head_dim, H*W).permute(0,1,3,2)\n",
    "        att = torch.matmul(q, k) / math.sqrt(self.head_dim)\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        out = torch.matmul(att, v)\n",
    "        out = out.permute(0,1,3,2).reshape(B, C, H, W)\n",
    "        return x + self.proj(out)\n",
    "\n",
    "class ResidualUpBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=4, s=2, p=1):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=k, stride=s, padding=p, bias=False)\n",
    "        self.res = ResidualConvBlock(out_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.res(self.up(x))\n",
    "\n",
    "class EncoderBridge4x4(nn.Module):\n",
    "    def __init__(self, in_ch, latent_dim):\n",
    "        super().__init__()\n",
    "        self.attn = AttentionBlock(in_ch)\n",
    "        self.conv = nn.Conv2d(in_ch, latent_dim, kernel_size=4, bias=False)\n",
    "        self.act  = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn(x)\n",
    "        x = self.conv(x)\n",
    "        return self.act(x)\n",
    "\n",
    "class DecoderBridge4x4(nn.Module):\n",
    "    def __init__(self, latent_dim, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose2d(latent_dim, out_ch, kernel_size=4, stride=4, bias=False)\n",
    "        self.attn = AttentionBlock(out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return self.attn(x)\n",
    "\n",
    "class PowerfulAutoencoder128_4x4(nn.Module):\n",
    "    def __init__(self, in_ch=1, latent_dim=2048,\n",
    "                 channels_mult=(1,2,4,8,8), base_ch=64):\n",
    "        super().__init__()\n",
    "        # Encoder: 128→4\n",
    "        self.enc0 = ResidualConvBlock(in_ch, base_ch)\n",
    "        self.enc1 = ResidualConvBlock(base_ch, base_ch*channels_mult[0], s=2)\n",
    "        self.enc2 = ResidualConvBlock(base_ch*channels_mult[0], base_ch*channels_mult[1], s=2)\n",
    "        self.enc3 = ResidualConvBlock(base_ch*channels_mult[1], base_ch*channels_mult[2], s=2)\n",
    "        self.enc4 = ResidualConvBlock(base_ch*channels_mult[2], base_ch*channels_mult[3], s=2)\n",
    "        self.enc5 = ResidualConvBlock(base_ch*channels_mult[3], base_ch*channels_mult[4], s=2)\n",
    "        ch = base_ch * channels_mult[-1]\n",
    "\n",
    "        self.attn           = AttentionBlock(ch)\n",
    "        self.encoder_bridge = EncoderBridge4x4(in_ch=ch, latent_dim=latent_dim)\n",
    "        self.decoder_bridge = DecoderBridge4x4(latent_dim=latent_dim, out_ch=ch)\n",
    "\n",
    "        # Decoder: 4→128 with ResidualUpBlock\n",
    "        self.up0 = ResidualUpBlock(ch, base_ch*channels_mult[3])  # 4→8\n",
    "        self.up1 = ResidualUpBlock(base_ch*channels_mult[3], base_ch*channels_mult[2])  # 8→16\n",
    "        self.up2 = ResidualUpBlock(base_ch*channels_mult[2], base_ch*channels_mult[1])  # 16→32\n",
    "        self.up3 = ResidualUpBlock(base_ch*channels_mult[1], base_ch*channels_mult[0])  # 32→64\n",
    "        self.up4 = ResidualUpBlock(base_ch*channels_mult[0], base_ch)                   # 64→128\n",
    "        self.final_conv = nn.Conv2d(base_ch, in_ch, 3, 1, 1)\n",
    "\n",
    "    def encoder(self, x):\n",
    "        x = self.enc0(x)\n",
    "        x = self.enc1(x)\n",
    "        x = self.enc2(x)\n",
    "        x = self.enc3(x)\n",
    "        x = self.enc4(x)\n",
    "        x = self.enc5(x)\n",
    "        # x = self.attn(x)\n",
    "        # x = self.encoder_bridge(x)\n",
    "        # x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "    def decode(self, z):\n",
    "        # x = z.reshape(-1, z.size(1), 1, 1)\n",
    "        x = z\n",
    "        # x = self.decoder_bridge(x)\n",
    "        x = self.up0(x)\n",
    "        x = self.up1(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.up3(x)\n",
    "        x = self.up4(x)\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        recon = self.decode(z)\n",
    "        return recon, z\n",
    "\n",
    "model = PowerfulAutoencoder128_4x4(in_ch=1, latent_dim=1024, base_ch = 16)\n",
    "x = torch.randn(8, 1, 128, 128)  # Batch of 8 images with 1 channel, 128x128 size\n",
    "recon, z = model(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Latent vector shape:\", z.shape)\n",
    "print(\"Reconstruction shape:\", recon.shape)\n",
    "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "params_in_million = params / 1e6\n",
    "print(f\"Total trainable parameters: {params_in_million:.2f} million\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([8, 1, 128, 128])\n",
      "Latent vector shape: torch.Size([8, 1024, 1, 1])\n",
      "Reconstruction shape: torch.Size([8, 1, 128, 128])\n",
      "Total trainable parameters: 41.11 million\n",
      "Input shape: torch.Size([8, 1, 128, 128])\n",
      "Latent vector shape: torch.Size([8, 64, 4, 4])\n",
      "Reconstruction shape: torch.Size([8, 1, 128, 128])\n",
      "Total trainable parameters: 56.36 million\n",
      "Input shape: torch.Size([8, 1, 128, 128])\n",
      "Latent vector shape: torch.Size([8, 16, 8, 8])\n",
      "Reconstruction shape: torch.Size([8, 1, 128, 128])\n",
      "Total trainable parameters: 55.67 million\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=3, s=1, p=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, k, s, p, bias=False)\n",
    "        self.norm1 = nn.GroupNorm(min(32, out_ch), out_ch)\n",
    "        self.act1 = nn.GELU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, k, 1, p, bias=False)\n",
    "        self.norm2 = nn.GroupNorm(min(32, out_ch), out_ch)\n",
    "        self.act2 = nn.GELU()\n",
    "        self.shortcut = (\n",
    "            nn.Conv2d(in_ch, out_ch, 1, s, bias=False)\n",
    "            if in_ch != out_ch or s != 1 else nn.Identity()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        res = self.shortcut(x)\n",
    "        out = self.act1(self.norm1(self.conv1(x)))\n",
    "        out = self.norm2(self.conv2(out))\n",
    "        return self.act2(out + res)\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, channels, num_heads=8):\n",
    "        super().__init__()\n",
    "        assert channels % num_heads == 0\n",
    "        self.norm = nn.GroupNorm(min(32, channels), channels)\n",
    "        self.q = nn.Conv2d(channels, channels, 1)\n",
    "        self.k = nn.Conv2d(channels, channels, 1)\n",
    "        self.v = nn.Conv2d(channels, channels, 1)\n",
    "        self.proj = nn.Conv2d(channels, channels, 1)\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = channels // num_heads\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        h = self.norm(x)\n",
    "        q = self.q(h).reshape(B, self.num_heads, self.head_dim, H*W).permute(0,1,3,2)\n",
    "        k = self.k(h).reshape(B, self.num_heads, self.head_dim, H*W).permute(0,1,2,3)\n",
    "        v = self.v(h).reshape(B, self.num_heads, self.head_dim, H*W).permute(0,1,3,2)\n",
    "        att = torch.matmul(q, k) / math.sqrt(self.head_dim)\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        out = torch.matmul(att, v).permute(0,1,3,2).reshape(B,C,H,W)\n",
    "        return x + self.proj(out)\n",
    "\n",
    "class ResidualUpBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=4, s=2, p=1):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_ch, out_ch, k, s, p, bias=False)\n",
    "        self.res = ResidualConvBlock(out_ch, out_ch)\n",
    "    def forward(self, x):\n",
    "        return self.res(self.up(x))\n",
    "\n",
    "# Variant 1: z shape = (B,1024,1,1), downsample via ResidualConvBlock to 4x4\n",
    "class AutoencoderZ1024(nn.Module):\n",
    "    def __init__(self, in_ch=1, base_ch=64, latent_ch=1024):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            ResidualConvBlock(in_ch, base_ch),          # 128→128\n",
    "            ResidualConvBlock(base_ch, base_ch, s=2),    # 128→64\n",
    "            ResidualConvBlock(base_ch, base_ch*2, s=2),  # 64→32\n",
    "            ResidualConvBlock(base_ch*2, base_ch*4, s=2),# 32→16\n",
    "            ResidualConvBlock(base_ch*4, base_ch*8, s=2),# 16→8\n",
    "            ResidualConvBlock(base_ch*8, base_ch*8, s=2) # 8→4\n",
    "        )\n",
    "        self.attn = AttentionBlock(base_ch*8)\n",
    "        self.to_latent   = nn.Conv2d(base_ch*8, latent_ch, 4, bias=False)          # 4→1\n",
    "        self.from_latent = nn.ConvTranspose2d(latent_ch, base_ch*8, 4, stride=4, bias=False) # 1→4\n",
    "        self.dec = nn.Sequential(\n",
    "            ResidualUpBlock(base_ch*8, base_ch*8),  # 4→8\n",
    "            ResidualUpBlock(base_ch*8, base_ch*4),  # 8→16\n",
    "            ResidualUpBlock(base_ch*4, base_ch*2),  # 16→32\n",
    "            ResidualUpBlock(base_ch*2, base_ch),    # 32→64\n",
    "            ResidualUpBlock(base_ch, base_ch),      # 64→128\n",
    "            nn.Conv2d(base_ch, in_ch, 3,1,1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.enc(x)\n",
    "        x = self.attn(x)\n",
    "        z = self.to_latent(x)     # (B,latent,1,1)\n",
    "        x = self.from_latent(z)\n",
    "        x = self.attn(x)\n",
    "        recon = self.dec(x)\n",
    "        return recon, z\n",
    "\n",
    "# Variant 2: z shape = (B,64,4,4), downsample to 4x4 via ResidualConvBlock\n",
    "class AutoencoderZ64(nn.Module):\n",
    "    def __init__(self, in_ch=1, base_ch=64, latent_ch=64):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            ResidualConvBlock(in_ch, base_ch),        # 64→64\n",
    "            ResidualConvBlock(base_ch, base_ch, s = 2),        # 64→64\n",
    "            ResidualConvBlock(base_ch, base_ch*2, s=2),  # 64→32\n",
    "            ResidualConvBlock(base_ch*2, base_ch*4, s=2),# 32→16\n",
    "            ResidualConvBlock(base_ch*4, base_ch*8, s=2),# 16→8\n",
    "            ResidualConvBlock(base_ch*8, base_ch*16, s=2) # 8→4\n",
    "        )\n",
    "        self.attn = AttentionBlock(base_ch*16)\n",
    "        self.to_latent   = ResidualConvBlock(base_ch*16, latent_ch, k=3, s=1, p=1)\n",
    "        self.from_latent = ResidualConvBlock(latent_ch, base_ch*16, k=3, s=1, p=1)\n",
    "        self.dec = nn.Sequential(\n",
    "            AttentionBlock(base_ch*16),\n",
    "            ResidualUpBlock(base_ch*16, base_ch*8), # 4→8\n",
    "            ResidualUpBlock(base_ch*8, base_ch*4), # 8→16\n",
    "            ResidualUpBlock(base_ch*4, base_ch*2),   # 16→32\n",
    "            ResidualUpBlock(base_ch*2, base_ch),     # 32→64\n",
    "            ResidualUpBlock(base_ch, base_ch),\n",
    "            nn.Conv2d(base_ch, in_ch, 3,1,1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.enc(x)\n",
    "        x = self.attn(x)\n",
    "        z = self.to_latent(x)    # (B,latent,4,4)\n",
    "        x = self.from_latent(z)\n",
    "        x = self.attn(x)\n",
    "        recon = self.dec(x)\n",
    "        return recon, z\n",
    "\n",
    "class AutoencoderZ16(nn.Module):\n",
    "    def __init__(self, in_ch=1, base_ch=128, latent_ch=16):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            ResidualConvBlock(in_ch, base_ch),        # 64→64\n",
    "            ResidualConvBlock(base_ch, base_ch, s = 2),\n",
    "            ResidualConvBlock(base_ch, base_ch*2, s=2),  # 64→32\n",
    "            ResidualConvBlock(base_ch*2, base_ch*4, s=2),# 32→16\n",
    "            ResidualConvBlock(base_ch*4, base_ch*8, s=2),# 16→8\n",
    "        )\n",
    "        self.attn = AttentionBlock(base_ch*8)\n",
    "        self.to_latent   = ResidualConvBlock(base_ch*8, latent_ch, k=3, s=1, p=1)\n",
    "        self.from_latent = ResidualConvBlock(latent_ch, base_ch*8, k=3, s=1, p=1)\n",
    "        self.dec = nn.Sequential(\n",
    "            AttentionBlock(base_ch*8),\n",
    "            ResidualUpBlock(base_ch*8, base_ch*4), # 8→16\n",
    "            ResidualUpBlock(base_ch*4, base_ch*2),   # 16→32\n",
    "            ResidualUpBlock(base_ch*2, base_ch),     # 32→64\n",
    "            ResidualUpBlock(base_ch, base_ch),\n",
    "            nn.Conv2d(base_ch, in_ch, 3,1,1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.enc(x)\n",
    "        x = self.attn(x)\n",
    "        z = self.to_latent(x)    # (B,latent,4,4)\n",
    "        x = self.from_latent(z)\n",
    "        x = self.attn(x)\n",
    "        recon = self.dec(x)\n",
    "        return recon, z\n",
    "    \n",
    "models = [AutoencoderZ1024(), AutoencoderZ64(), AutoencoderZ16()]\n",
    "for model in models:\n",
    "    x = torch.randn(8, 1, 128, 128)  # Batch of 8 images with 1 channel, 128x128 size\n",
    "    recon, z = model(x)\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Latent vector shape:\", z.shape)\n",
    "    print(\"Reconstruction shape:\", recon.shape)\n",
    "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    params_in_million = params / 1e6\n",
    "    print(f\"Total trainable parameters: {params_in_million:.2f} million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total .ckpt size: 5.23 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_ckpt_total_size(root_dir):\n",
    "    total_size = 0\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for fname in filenames:\n",
    "            if fname.endswith('.ckpt'):\n",
    "                fpath = os.path.join(dirpath, fname)\n",
    "                total_size += os.path.getsize(fpath)\n",
    "    return total_size\n",
    "\n",
    "def format_size(bytes_size):\n",
    "    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if bytes_size < 1024:\n",
    "            return f\"{bytes_size:.2f} {unit}\"\n",
    "        bytes_size /= 1024\n",
    "    return f\"{bytes_size:.2f} PB\"\n",
    "\n",
    "root = \"/home/vatsal/NWM\"\n",
    "total = get_ckpt_total_size(root)\n",
    "print(\"Total .ckpt size:\", format_size(total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vatsal/NWM/weatherforecasting: 20.60 GB\n",
      "/home/vatsal/NWM/weather: 11.77 GB\n",
      "/home/vatsal/NWM/sevir_lr: 8.77 GB\n",
      "/home/vatsal/NWM/PreDiff: 0.36 GB\n",
      "/home/vatsal/NWM/DiffCast: 0.16 GB\n",
      "/home/vatsal/NWM/AlphaPre: 0.00 GB\n",
      "/home/vatsal/NWM/.vscode: 0.00 GB\n",
      "/home/vatsal/NWM/wandb: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    for dirpath, _, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fpath = os.path.join(dirpath, f)\n",
    "            try:\n",
    "                if not os.path.islink(fpath) and os.path.isfile(fpath):\n",
    "                    total += os.path.getsize(fpath)\n",
    "            except:\n",
    "                continue\n",
    "    return total\n",
    "\n",
    "def list_top_dirs(root_dir, top_n=20):\n",
    "    dir_sizes = []\n",
    "    for entry in os.scandir(root_dir):\n",
    "        if entry.is_dir(follow_symlinks=False):\n",
    "            size = get_dir_size(entry.path)\n",
    "            dir_sizes.append((entry.path, size))\n",
    "    \n",
    "    dir_sizes.sort(key=lambda x: x[1], reverse=True)\n",
    "    for path, size in dir_sizes[:top_n]:\n",
    "        print(f\"{path}: {size / (1024**3):.2f} GB\")\n",
    "\n",
    "# Example usage\n",
    "list_top_dirs(\"/home/vatsal/NWM\", top_n=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Input image shape: \t\ttorch.Size([4, 1, 128, 128])\n",
      "--------------------------------------------------\n",
      "--- Forward Pass Shapes ---\n",
      "1. After Encoder: \t\ttorch.Size([4, 64, 8, 8])\n",
      "2. After Positional Encoding: \ttorch.Size([4, 66, 8, 8])\n",
      "3. Flattened Latent Vector (z): torch.Size([4, 4224])\n",
      "4. Reconstructed Image: \ttorch.Size([4, 1, 128, 128])\n",
      "--------------------------------------------------\n",
      "Total trainable parameters: 124,673\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Adds x, y coordinate information to the feature map.\n",
    "    This is a non-learnable layer.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): The input feature map of shape (B, C, H, W).\n",
    "        Returns:\n",
    "            Tensor: The output tensor with 2 extra channels for x and y coords.\n",
    "                    Shape will be (B, C+2, H, W).\n",
    "        \"\"\"\n",
    "        # Get the shape of the input tensor\n",
    "        b, c, h, w = x.shape\n",
    "        \n",
    "        # Create coordinate grids\n",
    "        # y_coords are horizontal lines, x_coords are vertical lines\n",
    "        y_coords = torch.linspace(-1, 1, h, device=x.device).unsqueeze(1).repeat(1, w)\n",
    "        x_coords = torch.linspace(-1, 1, w, device=x.device).unsqueeze(0).repeat(h, 1)\n",
    "        \n",
    "        # Add a channel dimension and batch dimension\n",
    "        # Shape becomes (1, 1, H, W)\n",
    "        y_coords = y_coords.unsqueeze(0).unsqueeze(0)\n",
    "        x_coords = x_coords.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Repeat for the entire batch\n",
    "        # Shape becomes (B, 1, H, W)\n",
    "        y_coords = y_coords.repeat(b, 1, 1, 1)\n",
    "        x_coords = x_coords.repeat(b, 1, 1, 1)\n",
    "        \n",
    "        # Concatenate along the channel dimension\n",
    "        # (B, C, H, W) + (B, 1, H, W) + (B, 1, H, W) -> (B, C+2, H, W)\n",
    "        return torch.cat([x, y_coords, x_coords], dim=1)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes a 1x128x128 image down to a 64x8x8 feature map.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # Input: 1 x 128 x 128\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # -> 16 x 64 x 64\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # -> 32 x 32 x 32\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # -> 64 x 16 x 16\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # -> 64 x 8 x 8\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decodes the flattened latent vector back to a 1x128x128 image.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # The first layer processes the combined feature and positional channels\n",
    "        # It learns to interpret the positional data\n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=66, out_channels=64, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.upsample_model = nn.Sequential(\n",
    "            # Input: 64 x 8 x 8\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'), # -> 64 x 16 x 16\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'), # -> 64 x 32 x 32\n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.Upsample(scale_factor=2, mode='nearest'), # -> 32 x 64 x 64\n",
    "            nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'), # -> 16 x 128 x 128\n",
    "            # Final convolution to get back to 1 channel\n",
    "            nn.Conv2d(16, 1, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "            # Apply Sigmoid to scale output pixels between 0 and 1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Reshape the flattened vector back to its 3D form (including pos channels)\n",
    "        # Original size: 66 * 8 * 8 = 4224\n",
    "        # The -1 in batch size infers it from the input\n",
    "        x = z.reshape(-1, 66, 8, 8)\n",
    "        \n",
    "        # Process with the initial 1x1 convolution\n",
    "        x = self.initial_conv(x)\n",
    "        \n",
    "        # Pass through the upsampling layers\n",
    "        reconstructed_image = self.upsample_model(x)\n",
    "        return reconstructed_image\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The full autoencoder model combining the encoder, positional encoding,\n",
    "    flattening, and the decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.pos_encoder = PositionalEncoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Encode the input image\n",
    "        features = self.encoder(x)\n",
    "        \n",
    "        # 2. Add positional embeddings\n",
    "        features_with_pos = self.pos_encoder(features)\n",
    "        \n",
    "        # 3. Flatten the result to create the latent vector 'z'\n",
    "        # The .reshape() method flattens the tensor while keeping the batch dimension\n",
    "        z = features_with_pos.reshape(features_with_pos.size(0), -1)\n",
    "        \n",
    "        # 4. Decode the latent vector to reconstruct the image\n",
    "        reconstruction = self.decoder(z)\n",
    "        \n",
    "        return reconstruction, z, features, features_with_pos\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == '__main__':\n",
    "    # Check if a CUDA-enabled GPU is available, otherwise use CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize the autoencoder model and move it to the selected device\n",
    "    model = Autoencoder().to(device)\n",
    "    \n",
    "    # Create a dummy input image tensor (batch size = 4)\n",
    "    # The image is 1 channel (grayscale), 128x128 pixels\n",
    "    batch_size = 4\n",
    "    input_image = torch.randn(batch_size, 1, 128, 128).to(device)\n",
    "    \n",
    "    print(f\"Input image shape: \\t\\t{input_image.shape}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Pass the image through the model\n",
    "    reconstructed_image, latent_vector, encoded_features, features_pos = model(input_image)\n",
    "    \n",
    "    # Print the shapes at each stage to verify the architecture\n",
    "    print(\"--- Forward Pass Shapes ---\")\n",
    "    print(f\"1. After Encoder: \\t\\t{encoded_features.shape}\")\n",
    "    print(f\"2. After Positional Encoding: \\t{features_pos.shape}\")\n",
    "    print(f\"3. Flattened Latent Vector (z): {latent_vector.shape}\")\n",
    "    print(f\"4. Reconstructed Image: \\t{reconstructed_image.shape}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # You can also check the number of parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading: 100%|██████████| 1440/1440 [00:00<00:00, 2760.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting PCA on (36000, 16384)\n",
      "Need k=1062 PCs for 95 % variance\n",
      "Smallest 3-D latent tensor (H/r, W/r, c): (64, 64, 8) volume = 32768\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy as np, h5py, tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import tqdm\n",
    "HDF5_PATH   = '/home/vatsal/NWM/sevir_lr/data/vil/2017/SEVIR_VIL_RANDOMEVENTS_2017_0501_0831.h5'   \n",
    "DS_NAME     = 'vil'           \n",
    "SAMPLE_STEP = 1         \n",
    "PATCH_SIZE  = 128            \n",
    "GPU         = torch.cuda.is_available()\n",
    "# --------------------------------\n",
    "\n",
    "frames = []\n",
    "with h5py.File(HDF5_PATH, 'r') as f:\n",
    "    imgs = f[DS_NAME][:]                     # (T, H, W) or (T, C, H, W)\n",
    "    if imgs.ndim == 4 and imgs.shape[1] == 1:\n",
    "        imgs = imgs[:, 0]\n",
    "    imgs = imgs[::SAMPLE_STEP]\n",
    "    for t in tqdm.tqdm(imgs, desc='loading'):\n",
    "        for x in range(0, t.shape[-1]):\n",
    "            img = t[..., x]\n",
    "            frames.append(img.flatten())\n",
    "X = np.stack(frames)        # (N, 16384) for 128²\n",
    "\n",
    "# 2. Exact PCA on GPU (or CPU)\n",
    "print('Fitting PCA on', X.shape)\n",
    "pca = PCA().fit(X)\n",
    "\n",
    "# 3. Energy curve\n",
    "cum = np.cumsum(pca.explained_variance_ratio_)\n",
    "k95 = int(np.searchsorted(cum, 0.95)) + 1\n",
    "print(f'Need k={k95} PCs for 95 % variance')\n",
    "\n",
    "# 4. Latent tensor bound\n",
    "H, W = PATCH_SIZE, PATCH_SIZE\n",
    "# Empirical rule: latent volume ≥ k95\n",
    "# Let r = spatial downsample ratio, c = channels\n",
    "def smallest_latent(k, r_list=[2,4,8]):\n",
    "    for r in r_list:\n",
    "        for c in [8,16,32,64]:\n",
    "            if (H//r)*(W//r)*c >= k:\n",
    "                return (H//r, W//r, c), (H//r)*(W//r)*c\n",
    "    return None, None\n",
    "shape, vol = smallest_latent(k95)\n",
    "print('Smallest 3-D latent tensor (H/r, W/r, c):', shape, 'volume =', vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H=64, W=64, c=8\n",
      "H=64, W=64, c=16\n",
      "H=64, W=64, c=32\n",
      "H=64, W=64, c=64\n",
      "H=64, W=64, c=128\n",
      "H=64, W=64, c=256\n",
      "H=32, W=32, c=8\n",
      "H=32, W=32, c=16\n",
      "H=32, W=32, c=32\n",
      "H=32, W=32, c=64\n",
      "H=32, W=32, c=128\n",
      "H=32, W=32, c=256\n",
      "H=16, W=16, c=8\n",
      "H=16, W=16, c=16\n",
      "H=16, W=16, c=32\n",
      "H=16, W=16, c=64\n",
      "H=16, W=16, c=128\n",
      "H=16, W=16, c=256\n",
      "H=8, W=8, c=32\n",
      "H=8, W=8, c=64\n",
      "H=8, W=8, c=128\n",
      "H=8, W=8, c=256\n",
      "H=4, W=4, c=128\n",
      "H=4, W=4, c=256\n",
      "Smallest 3-D latent tensor (H/r, W/r, c): None volume = None\n"
     ]
    }
   ],
   "source": [
    "H, W = PATCH_SIZE, PATCH_SIZE\n",
    "# Empirical rule: latent volume ≥ k95\n",
    "# Let r = spatial downsample ratio, c = channels\n",
    "def smallest_latent(k, r_list=[2,4,8,16,32,64,128]):\n",
    "    for r in r_list:\n",
    "        for c in [8,16,32,64,128,256]:\n",
    "            if (H//r)*(W//r)*c >= k:\n",
    "                print(f\"H={H//r}, W={W//r}, c={c}\")\n",
    "                # return (H//r, W//r, c), (H//r)*(W//r)*c\n",
    "    return None, None\n",
    "shape, vol = smallest_latent(k95)\n",
    "print('Smallest 3-D latent tensor (H/r, W/r, c):', shape, 'volume =', vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([2, 3, 128, 128])\n",
      "trainable params: 17.2 M\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: torch.Size([2, 1, 128, 128])\n",
      "latent: torch.Size([2, 4096])\n",
      "trainable params: 25.5 M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Pre-Act Bottleneck (no SE, simple, stable)\n",
    "# channels -> channels, 3×3 grouped conv\n",
    "# ------------------------------------------------------------\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, channels: int, groups: int = 8):\n",
    "        super().__init__()\n",
    "        mid = channels // 4\n",
    "        # ensure groups divides mid\n",
    "        g = min(groups, mid)\n",
    "        assert mid % g == 0, f\"groups ({g}) must divide mid channels ({mid})\"\n",
    "        self.f = nn.Sequential(\n",
    "            nn.BatchNorm2d(channels), nn.GELU(),\n",
    "            nn.Conv2d(channels, mid, 1, bias=False),\n",
    "            nn.BatchNorm2d(mid), nn.GELU(),\n",
    "            nn.Conv2d(mid, mid, 3, padding=1, groups=g, bias=False),\n",
    "            nn.BatchNorm2d(mid), nn.GELU(),\n",
    "            nn.Conv2d(mid, channels, 1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.f(x)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Encoder block: stride-2 conv + N Bottlenecks\n",
    "# ------------------------------------------------------------\n",
    "class EncBlock(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, num_blocks: int = 2, groups: int = 8):\n",
    "        super().__init__()\n",
    "        self.down = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch), nn.GELU()\n",
    "        )\n",
    "        self.res = nn.Sequential(*[Bottleneck(out_ch, groups=groups) for _ in range(num_blocks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.res(self.down(x))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Decoder block: stride-2 transposed conv + N Bottlenecks\n",
    "# ------------------------------------------------------------\n",
    "class DecBlock(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, num_blocks: int = 2, groups: int = 8):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_ch, out_ch, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch), nn.GELU()\n",
    "        )\n",
    "        self.res = nn.Sequential(*[Bottleneck(out_ch, groups=groups) for _ in range(num_blocks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.res(self.up(x))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Full AE with optional activation\n",
    "# ------------------------------------------------------------\n",
    "class PosAwareAutoEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 1,\n",
    "        latent_channels: int = 16,\n",
    "        groups: int = 8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.latent_channels = latent_channels\n",
    "\n",
    "        # ------------- Encoder -------------\n",
    "        self.enc = nn.Sequential(\n",
    "            EncBlock(in_channels, 256, num_blocks=4, groups=groups),\n",
    "            EncBlock(256, 512, num_blocks=4, groups=groups),\n",
    "            EncBlock(512, 1024, num_blocks=4, groups=groups),\n",
    "            nn.Conv2d(1024, latent_channels, 1)\n",
    "        )\n",
    "\n",
    "        # ------------- Positional embedding -------------\n",
    "        self.pos_emb = nn.Parameter(torch.randn(latent_channels, 16, 16))\n",
    "\n",
    "        # ------------- Decoder -------------\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Conv2d(latent_channels, 1024, 1),\n",
    "            DecBlock(1024, 512, num_blocks=4, groups=groups),\n",
    "            DecBlock(512, 256, num_blocks=4, groups=groups),\n",
    "            DecBlock(256, 128, num_blocks=4, groups=groups),\n",
    "            nn.Conv2d(128, in_channels, 3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.act = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "        z = self.enc(x)\n",
    "        z = z + self.pos_emb\n",
    "        return z.flatten(1)\n",
    "\n",
    "    def decode(self, z_flat):\n",
    "        z = z_flat.view(-1, self.latent_channels, 16, 16)\n",
    "        return self.act(self.dec(z))\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z), z\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Quick test\n",
    "# ------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    net = PosAwareAutoEncoder()\n",
    "    x = torch.randn(2, 1, 128, 128)\n",
    "    y, z = net(x)\n",
    "    print(\"output:\", y.shape)\n",
    "    print(\"latent:\", z.shape)\n",
    "    params = sum(p.numel() for p in net.parameters()) / 1e6\n",
    "    print(f\"trainable params: {params:.1f} M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "--- Model Test ---\n",
      "Final Latent Dimension (z.shape[1]): 4096\n",
      "Output shape: torch.Size([2, 1, 128, 128])\n",
      "Latent shape: torch.Size([2, 4096])\n",
      "Total trainable params: 29.88 M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CNN building blocks (unchanged)\n",
    "# ------------------------------------------------------------\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, channels: int, groups: int = 8):\n",
    "        super().__init__()\n",
    "        mid = channels // 4\n",
    "        g = min(groups, mid)\n",
    "        if g <= 0: g = 1\n",
    "        while mid % g != 0: g -= 1\n",
    "        if g <= 0: g = 1\n",
    "        self.f = nn.Sequential(\n",
    "            nn.BatchNorm2d(channels), nn.GELU(),\n",
    "            nn.Conv2d(channels, mid, 1, bias=False),\n",
    "            nn.BatchNorm2d(mid), nn.GELU(),\n",
    "            nn.Conv2d(mid, mid, 3, padding=1, groups=g, bias=False),\n",
    "            nn.BatchNorm2d(mid), nn.GELU(),\n",
    "            nn.Conv2d(mid, channels, 1, bias=False)\n",
    "        )\n",
    "    def forward(self, x): return x + self.f(x)\n",
    "\n",
    "class EncBlock(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, num_blocks: int = 2, groups: int = 8):\n",
    "        super().__init__()\n",
    "        self.down = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch), nn.GELU()\n",
    "        )\n",
    "        self.res = nn.Sequential(*[Bottleneck(out_ch, groups=groups) for _ in range(num_blocks)])\n",
    "    def forward(self, x): return self.res(self.down(x))\n",
    "\n",
    "class DecBlock(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, num_blocks: int = 2, groups: int = 8):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_ch, out_ch, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch), nn.GELU()\n",
    "        )\n",
    "        self.res = nn.Sequential(*[Bottleneck(out_ch, groups=groups) for _ in range(num_blocks)])\n",
    "    def forward(self, x): return self.res(self.up(x))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# The Final, Balanced Hybrid CNN-Transformer Autoencoder\n",
    "# ------------------------------------------------------------\n",
    "class HybridAE_8x8_Grid(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 1,\n",
    "        img_size: int = 128,\n",
    "        # We will create a grid of [64, 8, 8] to get our 4096 latent vector\n",
    "        latent_channels: int = 64,\n",
    "        # CNN goes deeper to reduce spatial size\n",
    "        cnn_channels: list = [64, 128, 256, 512],\n",
    "        cnn_blocks: int = 4,\n",
    "        # Transformer parameters\n",
    "        tr_depth: int = 8,\n",
    "        tr_heads: int = 8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.latent_channels = latent_channels\n",
    "        self.cnn_final_channels = cnn_channels[-1]\n",
    "\n",
    "        # Calculate grid size after deeper CNN encoding\n",
    "        self.grid_size = img_size // (2 ** len(cnn_channels)) # 128 / (2^4) = 8\n",
    "        self.n_tokens = self.grid_size ** 2 # 8*8 = 64\n",
    "\n",
    "        # The final 1D latent vector size is now correct by design\n",
    "        self.latent_dim = self.n_tokens * self.latent_channels # 64 * 64 = 4096\n",
    "        \n",
    "        # ------------- 1. Deeper CNN Encoder Head -------------\n",
    "        enc_layers = []\n",
    "        ch_in = in_channels\n",
    "        for ch_out in cnn_channels:\n",
    "            enc_layers.append(EncBlock(ch_in, ch_out, num_blocks=cnn_blocks))\n",
    "            ch_in = ch_out\n",
    "        self.cnn_encoder = nn.Sequential(*enc_layers)\n",
    "        # Output is now [B, 512, 8, 8]\n",
    "\n",
    "        # ------------- 2. Projection to Latent Channel Dimension -------------\n",
    "        self.to_latent_channels = nn.Conv2d(self.cnn_final_channels, self.latent_channels, 1)\n",
    "        # Output is now [B, 64, 8, 8]\n",
    "\n",
    "        # ------------- 3. Transformer Encoder Body -------------\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.n_tokens, self.latent_channels))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.latent_channels, nhead=tr_heads, dim_feedforward=self.latent_channels * 4,\n",
    "            dropout=0.1, activation='gelu', batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=tr_depth)\n",
    "\n",
    "        # ------------- 4. Transformer Decoder Body -------------\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, self.n_tokens, self.latent_channels))\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=self.latent_channels, nhead=tr_heads, dim_feedforward=self.latent_channels * 4,\n",
    "            dropout=0.1, activation='gelu', batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=tr_depth)\n",
    "\n",
    "        # ------------- 5. Projection from Latent Channel Dimension -------------\n",
    "        self.from_latent_channels = nn.Conv2d(self.latent_channels, self.cnn_final_channels, 1)\n",
    "        \n",
    "        # ------------- 6. Deeper CNN Decoder Tail -------------\n",
    "        dec_layers = []\n",
    "        ch_in = self.cnn_final_channels\n",
    "        for ch_out in reversed(cnn_channels[:-1]):\n",
    "            dec_layers.append(DecBlock(ch_in, ch_out, num_blocks=cnn_blocks))\n",
    "            ch_in = ch_out\n",
    "        dec_layers.append(DecBlock(cnn_channels[0], cnn_channels[0], num_blocks=cnn_blocks))\n",
    "        self.cnn_decoder = nn.Sequential(*dec_layers)\n",
    "\n",
    "        self.output_conv = nn.Conv2d(cnn_channels[0], in_channels, 3, padding=1)\n",
    "        self.act = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "        # 1. CNN downsampling to a small spatial grid\n",
    "        x = self.cnn_encoder(x) # -> [B, 512, 8, 8]\n",
    "\n",
    "        # 2. Project to the target latent channel dimension\n",
    "        x = self.to_latent_channels(x) # -> [B, 64, 8, 8]\n",
    "\n",
    "        # 3. Reshape for Transformer and add position embedding\n",
    "        x = x.flatten(2).transpose(1, 2) # -> [B, 64, 64]\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        # 4. Process sequence with Transformer\n",
    "        x = self.transformer_encoder(x) # -> [B, 64, 64]\n",
    "\n",
    "        # 5. Flatten to create the final 1D latent vector 'z'\n",
    "        z = x.flatten(1) # -> [B, 64 * 64] = [B, 4096]\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        # 1. Reshape 'z' from 1D vector back to a sequence\n",
    "        x = z.view(-1, self.n_tokens, self.latent_channels) # -> [B, 64, 64]\n",
    "\n",
    "        # 2. Transformer Decoder\n",
    "        memory = x\n",
    "        decoder_input = torch.zeros_like(x, device=z.device)\n",
    "        decoder_input = decoder_input + self.decoder_pos_embed\n",
    "        x = self.transformer_decoder(tgt=decoder_input, memory=memory) # -> [B, 64, 64]\n",
    "\n",
    "        # 3. Reshape back to an image-like grid\n",
    "        x = x.transpose(1, 2).view(-1, self.latent_channels, self.grid_size, self.grid_size) # -> [B, 64, 8, 8]\n",
    "\n",
    "        # 4. Project back to the original CNN channel dimension\n",
    "        x = self.from_latent_channels(x) # -> [B, 512, 8, 8]\n",
    "\n",
    "        # 5. CNN upsampling\n",
    "        x = self.cnn_decoder(x) # -> [B, 64, 128, 128]\n",
    "\n",
    "        x = self.output_conv(x) # -> [B, 1, 128, 128]\n",
    "        return self.act(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        recon = self.decode(z)\n",
    "        return recon, z\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Quick test\n",
    "# ------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    net = HybridAE_8x8_Grid(\n",
    "        latent_channels = 64,      # unchanged\n",
    "        # --------------  W I D E R   C N N  -----------------\n",
    "        cnn_channels = [128, 256, 512, 1024],   # 2× wider\n",
    "        cnn_blocks = 6,                          # deeper too\n",
    "        # --------------  T R A N S F O R M E R  -------------\n",
    "        tr_depth = 12,                           # deeper\n",
    "        tr_heads = 8,   \n",
    "    ).to(device)\n",
    "\n",
    "    x = torch.randn(2, 1, 128, 128).to(device)\n",
    "    y, z = net(x)\n",
    "\n",
    "    print(f\"\\n--- Model Test ---\")\n",
    "    print(\"Final Latent Dimension (z.shape[1]):\", z.shape[1])\n",
    "    print(\"Output shape:\", y.shape)\n",
    "    print(\"Latent shape:\", z.shape)\n",
    "\n",
    "    params = sum(p.numel() for p in net.parameters() if p.requires_grad) / 1e6\n",
    "    print(f\"Total trainable params: {params:.2f} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earthformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
